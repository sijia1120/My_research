{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2a and 2b: Non-arbitrage Model\n",
    "\n",
    "1. Model 2a: 49 firm-level characteristics + 8 macro-economic variable \n",
    "2. Model 2b: 49 firm-level characteristics + 134 macro-economic variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras_tuner import HyperModel\n",
    "import keras_tuner as kt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#!pip install keras_tuner\n",
    "#import keras_tuner\n",
    "\n",
    "import keras\n",
    "from keras import models, layers, metrics, Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Input, Dense, Multiply, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import keras_tuner\n",
    "from keras_tuner import HyperModel\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "import pandas as pd\n",
    "path = 'dataset/'\n",
    "firm_df = pd.read_pickle(path+'firm_df49.pkl')\n",
    "firm_df.index = pd.to_datetime(firm_df.index).to_period('M')\n",
    "\n",
    "# macro-economic variables\n",
    "# There is one features which has 37 missing values but before year 1980\n",
    "macro_df = pd.read_pickle(path+'macro_df.pkl')\n",
    "macro_df.index = pd.to_datetime(macro_df.index, format='%m/%d/%Y')\n",
    "macro_df.rename(columns={\"ep\": \"ep_macro\"}, inplace=True)\n",
    "macro_index = macro_df.index[1:]\n",
    "macro_new = macro_df.iloc[:-1, :]\n",
    "macro_new.index = macro_index\n",
    "macro_new.index = macro_new.index.to_period('M')\n",
    "\n",
    "# Scale macro-features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "#macro_scaled = scaler.fit_transform(macro_new)\n",
    "#macro_scaled_df = pd.DataFrame(macro_scaled, columns=macro_new.columns)\n",
    "#macro_scaled_df.index = macro_new.index\n",
    "\n",
    "# merged data 1\n",
    "merged_df = pd.merge(firm_df, macro_new, left_on='jdate', right_on='sasdate', how='inner')\n",
    "merged_df.index = firm_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_core = ['acc','agr','beta','bm','cash','cashpr','cfp','chatoia','chcsho','chfeps','chinv',\n",
    "       'chmom','chpmia','chtx','currat','depr','dy','ear','ep','gma','grcapx','grltnoa',\n",
    "       'ill','indmom','invest','lev','lgr','maxret','mom12m','mom1m','mom36m','mve','nincr',\n",
    "       'orgcap','pchgm_pchsale','pchsale_pchinvt','pchsale_pchrect','pchsale_pchxsga',\n",
    "       'retvol','roaq','roavol','roeq','salecash','saleinv','sgr','sp','std_dolvol','std_turn','turn']\n",
    "\n",
    "info_list = ['fyear','year','jyear','permno','ticker','comnam','exchcd','exchname','siccd',\n",
    "       'indname','size_class','mve_m','rf','ret','ret_adj','ret_ex','ret_adj_ex',]\n",
    "\n",
    "macro_col = ['RPI', 'W875RX1', 'DPCERA3M086SBEA', 'CMRMTSPLx', 'RETAILx','INDPRO', 'IPFPNSS', 'IPFINAL', 'IPCONGD', 'IPDCONGD', 'IPNCONGD',\n",
    "       'IPBUSEQ', 'IPMAT', 'IPDMAT', 'IPNMAT', 'IPMANSICS', 'IPB51222S','IPFUELS', 'CUMFNS', 'HWI', 'HWIURATIO', 'CLF16OV', 'CE16OV',\n",
    "       'UNRATE', 'UEMPMEAN', 'UEMPLT5', 'UEMP5TO14', 'UEMP15OV','UEMP15T26', 'UEMP27OV', 'CLAIMSx', 'PAYEMS', 'USGOOD',\n",
    "       'CES1021000001', 'USCONS', 'MANEMP', 'DMANEMP', 'NDMANEMP','SRVPRD', 'USTPU', 'USWTRADE', 'USTRADE', 'USFIRE', 'USGOVT',\n",
    "       'CES0600000007', 'AWOTMAN', 'AWHMAN', 'HOUST', 'HOUSTNE','HOUSTMW', 'HOUSTS', 'HOUSTW', 'PERMIT', 'PERMITNE', 'PERMITMW',\n",
    "       'PERMITS', 'PERMITW', 'AMDMNOx', 'ANDENOx', 'AMDMUOx', 'BUSINVx','ISRATIOx', 'M1SL', 'M2SL', 'M2REAL', 'BOGMBASE', 'TOTRESNS',\n",
    "       'NONBORRES', 'BUSLOANS', 'REALLN', 'NONREVSL', 'CONSPI', 'S&P 500','S&P: indust', 'S&P div yield', 'S&P PE ratio', 'FEDFUNDS',\n",
    "       'CP3Mx', 'TB3MS', 'TB6MS', 'GS1', 'GS5', 'GS10', 'AAA', 'BAA','COMPAPFFx', 'TB3SMFFM', 'TB6SMFFM', 'T1YFFM', 'T5YFFM', 'T10YFFM',\n",
    "       'AAAFFM', 'BAAFFM', 'TWEXAFEGSMTHx', 'EXSZUSx', 'EXJPUSx','EXUSUKx', 'EXCAUSx', 'WPSFD49207', 'WPSFD49502', 'WPSID61',\n",
    "       'WPSID62', 'OILPRICEx', 'PPICMM', 'CPIAUCSL', 'CPIAPPSL','CPITRNSL', 'CPIMEDSL', 'CUSR0000SAC', 'CUSR0000SAD',\n",
    "       'CUSR0000SAS', 'CPIULFSL', 'CUSR0000SA0L2', 'CUSR0000SA0L5','PCEPI', 'DDURRG3M086SBEA', 'DNDGRG3M086SBEA', 'DSERRG3M086SBEA',\n",
    "       'CES0600000008', 'CES2000000008', 'CES3000000008', 'UMCSENTx','DTCOLNVHFNM', 'DTCTHFNM', 'INVEST', 'VIXCLSx', 'dp', 'ep_macro', 'b/m',\n",
    "       'ntis', 'tbl', 'tms', 'dfy', 'svar']\n",
    "\n",
    "macro_core =  ['dp', 'ep_macro', 'b/m','ntis', 'tbl', 'tms', 'dfy', 'svar']\n",
    "\n",
    "def datasplit(df, dataset):\n",
    "    df_train = df[df.index < pd.Period((str(1995)+\"-1\"),freq='M')]\n",
    "    df_val = df[(df.index >= pd.Period((str(1995)+\"-1\"),freq='M')) & (df.index < pd.Period((str(2000)+\"-1\"),freq='M'))]\n",
    "    df_test = df[df.index >= pd.Period((str(2000)+\"-1\"),freq='M')]\n",
    "    if dataset == \"firm\":\n",
    "        X_train = df_train[char_core]\n",
    "        X_val = df_val[char_core]\n",
    "        X_test = df_test[char_core]\n",
    "    elif dataset == 'macro':\n",
    "        X_train = df_train[macro_col]\n",
    "        X_val = df_val[macro_col]\n",
    "        X_test = df_test[macro_col]\n",
    "    elif dataset == \"firm_macro\":\n",
    "        X_train = df_train[char_core+macro_col]\n",
    "        X_val = df_val[char_core+macro_col]\n",
    "        X_test = df_test[char_core+macro_col]\n",
    "    elif dataset == \"firm_macro8\":\n",
    "        X_train = df_train[char_core+macro_core]\n",
    "        X_val = df_val[char_core+macro_core]\n",
    "        X_test = df_test[char_core+macro_core] \n",
    "    y_train = df_train['predicted_return']\n",
    "    y_val = df_val['predicted_return']\n",
    "    y_test = df_test['predicted_return']\n",
    "    return (X_train, y_train, X_val, y_val, X_test, y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def my_metric_fn(y_true, y_pred):\n",
    "    num = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    den = tf.reduce_mean(tf.square(y_true - tf.zeros_like(y_true)))\n",
    "    return 1 - num / den\n",
    "\n",
    "def call_existing_code(input_value):\n",
    "    L = 3\n",
    "    l1_reg= 0.\n",
    "    l2_reg= 0.1\n",
    "    dropout_ =0.2\n",
    "    lr =0.0001\n",
    "    acti='relu'\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, input_dim=input_value, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg), activation=acti))\n",
    "    model.add(Dropout(dropout_))\n",
    "    model.add(Dense(units=32, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg), activation=acti))\n",
    "    model.add(Dropout(dropout_))\n",
    "    model.add(Dense(units=16, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg), activation=acti))\n",
    "    model.add(Dropout(dropout_))\n",
    "    model.add(Dense(units=8, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg), activation=acti))\n",
    "    model.add(Dropout(dropout_))\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[my_metric_fn])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric_fn(y_true, y_pred):\n",
    "    num = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    den = tf.reduce_mean(tf.square(y_true))\n",
    "    return 1 - num / den\n",
    "\n",
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def __init__(self,num_input):\n",
    "        self.num_input = num_input\n",
    "\n",
    "    def build(self, hp,):\n",
    "        l1_reg = hp.Choice(\"l1_ratio\", [0.])\n",
    "        l2_reg = hp.Choice(\"l2_ratio\", [0.001])\n",
    "        init_mode = 'glorot_normal'\n",
    "        layer = hp.Choice(\"layer\", [4])\n",
    "        acti = 'relu'\n",
    "        dropout = hp.Choice(\"dropout\",[0.1,])\n",
    "        lr = hp.Choice(\"learning_rate\", [0.0001])\n",
    "        model = keras.Sequential()\n",
    "        model.add(Dense(units=64, input_dim=self.num_input, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg), kernel_initializer=init_mode, activation=acti))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "        for i in range (layer-1):\n",
    "            model.add(Dense(int(32/2**i),kernel_regularizer=l1_l2(l1_reg,l2=l2_reg), kernel_initializer=init_mode, activation=acti))\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dense(1, kernel_initializer=init_mode)) \n",
    "        opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
    "        model.compile(loss='mean_squared_error', optimizer=opt,metrics=[my_metric_fn])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256]),\n",
    "            **kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49 Firm-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88347, 49) (88347,) (31851, 49) (31851,) (90007, 49) (90007,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = datasplit(firm_df, dataset = \"firm\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 20s]\n",
      "val_loss: 0.014586332254111767\n",
      "\n",
      "Best val_loss So Far: 0.014586332254111767\n",
      "Total elapsed time: 00h 00m 20s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(MyHyperModel(X_train.shape[1]),objective='val_loss',max_trials=10,overwrite=True,)\n",
    "tuner.search(X_train, y_train, epochs=50, \n",
    "             validation_data=(X_val, y_val), \n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Trials for Prediction\n",
    "x_all = np.concatenate((X_train, X_val))\n",
    "y_all = np.concatenate((y_train, y_val))\n",
    "seed_list = [458, 165, 530, 564, 590, 560, 829, 170, 376, 176]\n",
    "yhat_df = pd.DataFrame()\n",
    "\n",
    "for random_seed in seed_list:\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(x=x_all, y=y_all, epochs=20,verbose=0,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(monitor='loss',mode='min', patience=2)])\n",
    "    print(best_model.evaluate(X_test,y_test))\n",
    "    y_hat = best_model.predict(X_test).reshape(-1)\n",
    "    print(y_hat.mean(),y_hat.std())\n",
    "    yhat_df[random_seed] = y_hat\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 is 0.006558914378433256\n"
     ]
    }
   ],
   "source": [
    "# Print out Predictive R^2\n",
    "y_predict = yhat_df.mean(axis=1).values.reshape(-1)\n",
    "y_real = y_test\n",
    "a = np.mean(np.square(y_predict -  y_real))\n",
    "b = np.mean(np.square(y_real))\n",
    "R2 = 1-a/b\n",
    "print(f\"R^2 is {R2}\")\n",
    "\n",
    "path_y = 'predict/'\n",
    "yhat_df.to_pickle(path_y+\"model2c49.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49 Firm-level + 8 Macroeconomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88347, 57) (88347,) (31851, 57) (31851,) (90007, 57) (90007,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = datasplit(merged_df, dataset = \"firm_macro8\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric_fn(y_true, y_pred):\n",
    "    num = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    den = tf.reduce_mean(tf.square(y_true))\n",
    "    return 1 - num / den\n",
    "\n",
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def __init__(self,num_input):\n",
    "        self.num_input = num_input\n",
    "\n",
    "    def build(self, hp,):\n",
    "        l1_reg = hp.Choice(\"l1_ratio\", [0.])\n",
    "        l2_reg = hp.Choice(\"l2_ratio\", [0.001,0.])\n",
    "        init_mode = 'glorot_normal'\n",
    "        layer = hp.Choice(\"layer\", [4])\n",
    "        acti = 'relu'\n",
    "        dropout = hp.Choice(\"dropout\",[0.1,0.2])\n",
    "        lr = hp.Choice(\"learning_rate\", [0.0001])\n",
    "        model = keras.Sequential()\n",
    "        model.add(Dense(units=64, input_dim=self.num_input, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg), kernel_initializer=init_mode, activation=acti))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "        for i in range (layer-1):\n",
    "            model.add(Dense(int(32/2**i),kernel_regularizer=l1_l2(l1_reg,l2=l2_reg), kernel_initializer=init_mode, activation=acti))\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dense(1, kernel_initializer=init_mode)) \n",
    "        opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
    "        model.compile(loss='mean_squared_error', optimizer=opt,metrics=[my_metric_fn])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,128]),\n",
    "            **kwargs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 00m 12s]\n",
      "val_loss: 0.01454280223697424\n",
      "\n",
      "Best val_loss So Far: 0.014537124894559383\n",
      "Total elapsed time: 00h 02m 24s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(MyHyperModel(X_train.shape[1]),objective='val_loss',max_trials=10,overwrite=True,)\n",
    "tuner.search(X_train, y_train, epochs=50, \n",
    "             validation_data=(X_val, y_val), \n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Trials for Prediction\n",
    "x_all = np.concatenate((X_train, X_val))\n",
    "y_all = np.concatenate((y_train, y_val))\n",
    "seed_list = [458, 165, 530, 564, 590, 560, 829, 170, 376, 176]\n",
    "yhat_df = pd.DataFrame()\n",
    "\n",
    "for random_seed in seed_list:\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(x=x_all, y=y_all, epochs=20,verbose=0,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(monitor='loss',mode='min', patience=2)])\n",
    "    print(best_model.evaluate(X_test,y_test))\n",
    "    y_hat = best_model.predict(X_test).reshape(-1)\n",
    "    print(y_hat.mean(),y_hat.std())\n",
    "    yhat_df[random_seed] = y_hat\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 is 0.001230689943262031\n"
     ]
    }
   ],
   "source": [
    "# Print out Predictive R^2\n",
    "y_predict = yhat_df.mean(axis=1).values.reshape(-1)\n",
    "y_real = y_test\n",
    "a = np.mean(np.square(y_predict -  y_real))\n",
    "b = np.mean(np.square(y_real))\n",
    "R2 = 1-a/b\n",
    "print(f\"R^2 is {R2}\")\n",
    "\n",
    "path_y = 'predict/'\n",
    "yhat_df.to_pickle(path_y+\"model2a49_8.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49 Firm-level + 134 Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88347, 183) (88347,) (31851, 183) (31851,) (90007, 183) (90007,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = datasplit(merged_df, dataset = \"firm_macro\")\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric_fn(y_true, y_pred):\n",
    "    num = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    den = tf.reduce_mean(tf.square(y_true))\n",
    "    return 1 - num / den\n",
    "\n",
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def __init__(self,num_input):\n",
    "        self.num_input = num_input\n",
    "\n",
    "    def build(self, hp,):\n",
    "        l1_reg = hp.Choice(\"l1_ratio\", [0.])\n",
    "        l2_reg = hp.Choice(\"l2_ratio\", [0.001,0.])\n",
    "        init_mode = 'glorot_normal'\n",
    "        layer = hp.Choice(\"layer\", [4])\n",
    "        acti = 'relu'\n",
    "        dropout = hp.Choice(\"dropout\",[0.1,0.2])\n",
    "        lr = hp.Choice(\"learning_rate\", [0.0001])\n",
    "        model = keras.Sequential()\n",
    "        model.add(Dense(units=64, input_dim=self.num_input, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg), kernel_initializer=init_mode, activation=acti))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(BatchNormalization())\n",
    "        for i in range (layer-1):\n",
    "            model.add(Dense(int(32/2**i),kernel_regularizer=l1_l2(l1_reg,l2=l2_reg), kernel_initializer=init_mode, activation=acti))\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dense(1, kernel_initializer=init_mode)) \n",
    "        opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
    "        model.compile(loss='mean_squared_error', optimizer=opt,metrics=[my_metric_fn])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,128]),\n",
    "            **kwargs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 00m 11s]\n",
      "val_loss: 0.01448238268494606\n",
      "\n",
      "Best val_loss So Far: 0.01448238268494606\n",
      "Total elapsed time: 00h 02m 26s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(MyHyperModel(X_train.shape[1]),objective='val_loss',max_trials=10,overwrite=True,)\n",
    "tuner.search(X_train, y_train, epochs=50, \n",
    "             validation_data=(X_val, y_val), \n",
    "             callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Trials for Prediction\n",
    "x_all = np.concatenate((X_train, X_val))\n",
    "y_all = np.concatenate((y_train, y_val))\n",
    "seed_list = [458, 165, 530, 564, 590, 560, 829, 170, 376, 176]\n",
    "yhat_df = pd.DataFrame()\n",
    "\n",
    "for random_seed in seed_list:\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(x=x_all, y=y_all, epochs=20,verbose=0,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(monitor='loss',mode='min', patience=2)])\n",
    "    print(best_model.evaluate(X_test,y_test))\n",
    "    y_hat = best_model.predict(X_test).reshape(-1)\n",
    "    print(y_hat.mean(),y_hat.std())\n",
    "    yhat_df[random_seed] = y_hat\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 is -1.8604247827026152\n"
     ]
    }
   ],
   "source": [
    "# Print out Predictive R^2\n",
    "y_predict = yhat_df.mean(axis=1).values.reshape(-1)\n",
    "y_real = y_test\n",
    "a = np.mean(np.square(y_predict -  y_real))\n",
    "b = np.mean(np.square(y_real))\n",
    "R2 = 1-a/b\n",
    "print(f\"R^2 is {R2}\")\n",
    "\n",
    "path_y = 'predict/'\n",
    "yhat_df.to_pickle(path_y+\"model2b49_134.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
